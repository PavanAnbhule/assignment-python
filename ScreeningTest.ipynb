{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb3916a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Variable: petal_width\n",
      "Type of Regression: regression\n"
     ]
    }
   ],
   "source": [
    "# Q1) Read the target and type of regression to be run\n",
    "\n",
    "from docx import Document\n",
    "import json\n",
    "\n",
    "# Step 2: Load the Word file (ensure the file path is correct)\n",
    "doc_path = r\"C:\\Users\\pavan Anbhule\\Downloads\\DA_Assessment\\algoparams_from_ui1.docx\"  # Replace with the correct file path\n",
    "doc = Document(doc_path)\n",
    "\n",
    "# Step 3: Extract the JSON text from the Word file\n",
    "json_text = \"\"\n",
    "for paragraph in doc.paragraphs:\n",
    "    json_text += paragraph.text\n",
    "\n",
    "# Step 4: Parse the JSON text\n",
    "try:\n",
    "    # Parse the JSON content\n",
    "    data = json.loads(json_text)\n",
    "\n",
    "    # Step 5: Navigate to the target variable and regression type\n",
    "    target = data.get(\"design_state_data\", {}).get(\"target\", {}).get(\"target\", \"Not found\")\n",
    "    regression_type = data.get(\"design_state_data\", {}).get(\"target\", {}).get(\"type\", \"Not found\")\n",
    "\n",
    "    # Step 6: Print results\n",
    "    print(f\"Target Variable: {target}\")\n",
    "    print(f\"Type of Regression: {regression_type}\")\n",
    "\n",
    "except json.JSONDecodeError as e:\n",
    "    # Handle JSON parsing errors\n",
    "    print(\"The content in the Word file is not valid JSON.\")\n",
    "    print(f\"Error Details: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9186d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Reduction selected. Keeping top 5 features.\n"
     ]
    }
   ],
   "source": [
    "# 3) Compute feature reduction based on input. See the screenshot below where there can be No Reduction, Corr with Target, Tree-based, PCA. \n",
    "#    Please make sure you write code so that all options can work. If we rerun your code with a different Json it should work if we switch \n",
    "#    No Reduction to say PCA.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to apply feature reduction based on JSON configuration\n",
    "def apply_feature_reduction(data, target_column, feature_reduction_config):\n",
    "    # Extract feature reduction method from the config\n",
    "    method = feature_reduction_config.get(\"feature_reduction_method\")\n",
    "    \n",
    "    # Check if \"No Reduction\" is selected\n",
    "    if feature_reduction_config[\"No Reduction\"][\"is_selected\"]:\n",
    "        num_features_to_keep = feature_reduction_config[\"No Reduction\"][\"num_of_features_to_keep\"]\n",
    "        print(f\"No Reduction selected. Keeping top {num_features_to_keep} features.\")\n",
    "        return data.iloc[:, :num_features_to_keep]\n",
    "    \n",
    "    # Check if \"Correlation with Target\" is selected\n",
    "    elif feature_reduction_config[\"Correlation with target\"][\"is_selected\"]:\n",
    "        num_features_to_keep = feature_reduction_config[\"Correlation with target\"][\"num_of_features_to_keep\"]\n",
    "        print(f\"Correlation with Target selected. Keeping top {num_features_to_keep} correlated features.\")\n",
    "        return correlation_with_target(data, target_column, num_features_to_keep)\n",
    "    \n",
    "    # Check if \"Tree-based\" method is selected\n",
    "    elif feature_reduction_config[\"Tree_based\"][\"is_selected\"]:\n",
    "        num_features_to_keep = feature_reduction_config[\"Tree_based\"][\"num_of_features_to_keep\"]\n",
    "        print(f\"Tree-based selection method selected. Keeping top {num_features_to_keep} features based on importance.\")\n",
    "        return tree_based_selection(data, target_column, num_features_to_keep)\n",
    "    \n",
    "    # Check if \"Principal Component Analysis\" (PCA) is selected\n",
    "    elif feature_reduction_config[\"Principal Component Analysis\"][\"is_selected\"]:\n",
    "        num_features_to_keep = feature_reduction_config[\"Principal Component Analysis\"][\"num_of_features_to_keep\"]\n",
    "        print(f\"PCA selected. Keeping top {num_features_to_keep} principal components.\")\n",
    "        return pca_reduction(data, num_features_to_keep)\n",
    "    \n",
    "    # Return original data if no valid method is selected\n",
    "    print(\"No valid feature reduction method selected. Returning original data.\")\n",
    "    return data\n",
    "\n",
    "# Function for correlation-based feature selection\n",
    "def correlation_with_target(data, target_column, num_features_to_keep):\n",
    "    corr = data.corrwith(data[target_column]).abs()\n",
    "    top_features = corr.sort_values(ascending=False).head(num_features_to_keep).index\n",
    "    return data[top_features]\n",
    "\n",
    "# Function for tree-based feature selection (using Random Forest)\n",
    "def tree_based_selection(data, target_column, num_features_to_keep):\n",
    "    X = data.drop(columns=[target_column])\n",
    "    y = data[target_column]\n",
    "    \n",
    "    # Train a RandomForest model to rank features\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    feature_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "    top_features = feature_importances.sort_values(ascending=False).head(num_features_to_keep).index\n",
    "    return data[top_features]\n",
    "\n",
    "# Function for PCA-based feature reduction\n",
    "def pca_reduction(data, num_features_to_keep):\n",
    "    X = data.values\n",
    "    pca = PCA(n_components=num_features_to_keep)\n",
    "    principal_components = pca.fit_transform(X)\n",
    "    \n",
    "    # Get the reduced data frame\n",
    "    reduced_data = pd.DataFrame(principal_components)\n",
    "    return reduced_data\n",
    "\n",
    "# Sample data for testing\n",
    "# Assuming `df` is your input data frame, and 'target' is the target column in your dataset\n",
    "# data = pd.read_csv('your_data.csv')  # Load your dataset\n",
    "\n",
    "# Example of the JSON input provided\n",
    "feature_reduction_config = {\n",
    "    \"feature_reduction_method\": \"Correlation with target\",\n",
    "    \"No Reduction\": {\n",
    "        \"is_selected\": True,\n",
    "        \"num_of_features_to_keep\": 5\n",
    "    },\n",
    "    \"Correlation with target\": {\n",
    "        \"is_selected\": False,\n",
    "        \"num_of_features_to_keep\": 0\n",
    "    },\n",
    "    \"Tree_based\": {\n",
    "        \"is_selected\": False,\n",
    "        \"num_of_features_to_keep\": 0,\n",
    "        \"depth_of_trees\": 0,\n",
    "        \"num_of_trees\": 0\n",
    "    },\n",
    "    \"Principal Component Analysis\": {\n",
    "        \"is_selected\": False,\n",
    "        \"num_of_features_to_keep\": 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Example of feature reduction applied\n",
    "# Here, we assume you have a DataFrame `df` and the target column is 'target'\n",
    "# data = df\n",
    "target_column = 'target'  # Change this to your actual target column\n",
    "\n",
    "# Apply feature reduction based on config\n",
    "reduced_data = apply_feature_reduction(data, target_column, feature_reduction_config)\n",
    "\n",
    "# The reduced data can now be used for further processing, training, or analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e659218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# 2) Read the features (which are column names in the csv) and figure out what missing imputation\n",
    "#    needs to be applied and apply that to the columns loaded in a dataframe\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load your data into a pandas dataframe\n",
    "df = pd.read_csv(r\"C:\\Users\\pavan Anbhule\\Downloads\\DA_Assessment\\iris.csv\")\n",
    "\n",
    "# Step 2: Define the feature handling dictionary (corrected the syntax)\n",
    "feature_handling = {\n",
    "    \"sepal_length\": {\n",
    "        \"feature_name\": \"sepal_length\",\n",
    "        \"is_selected\": True,\n",
    "        \"feature_variable_type\": \"numerical\",\n",
    "        \"feature_details\": {\n",
    "            \"numerical_handling\": \"keep as regular numerical feature\",\n",
    "            \"rescaling\": \"No rescaling\",\n",
    "            \"make_derived_feats\": False,\n",
    "            \"missing_values\": \"Impute\",\n",
    "            \"impute_width\": \"Average of values\",\n",
    "            \"impute_value\": 0\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Step 3: Function to impute missing values based on the feature handling configuration\n",
    "def impute_missing_values(df, feature_handling):\n",
    "    for feature, details in feature_handling.items():\n",
    "        if details[\"is_selected\"]:  # Check if the feature is selected\n",
    "            column_name = details[\"feature_name\"]\n",
    "            if \"missing_values\" in details[\"feature_details\"]:\n",
    "                missing_values_action = details[\"feature_details\"][\"missing_values\"]\n",
    "                if missing_values_action == \"Impute\":\n",
    "                    impute_method = details[\"feature_details\"][\"impute_width\"]\n",
    "                    \n",
    "                    if impute_method == \"Average of values\":  # Check if impute method is mean\n",
    "                        # Calculate the mean of the column (ignoring NaN values)\n",
    "                        mean_value = df[column_name].mean()\n",
    "                        # Impute missing values with the mean value\n",
    "                        df[column_name].fillna(mean_value, inplace=True)\n",
    "                    # You can add more imputation methods if required (e.g., Median, Mode)\n",
    "    return df\n",
    "\n",
    "# Step 4: Apply the imputation function to your dataframe\n",
    "df = impute_missing_values(df, feature_handling)\n",
    "\n",
    "# Step 5: Verify that missing values have been imputed\n",
    "print(df['sepal_length'].isnull().sum())  # This should print 0 if imputation was successful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9fdcc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LogisticRegression trained.\n"
     ]
    }
   ],
   "source": [
    "# 4) Parse the Json and make the model objects (using sklean) that can handle what is required \n",
    "#   in the “prediction_type” specified in the JSON (See #1 where “prediction_type” is specified).\n",
    "#   Keep in mind not to pick models that don’t apply for the prediction_type specified\n",
    "\n",
    "import json\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample JSON configuration\n",
    "config = {\n",
    "    \"prediction_type\": \"classification\",\n",
    "    \"LogisticRegression\": {\n",
    "        \"model_name\": \"LogisticRegression\",\n",
    "        \"is_selected\": True,\n",
    "        \"parallelism\": 2,\n",
    "        \"min_iter\": 30,\n",
    "        \"max_iter\": 50,\n",
    "        \"min_regparam\": 0.5,\n",
    "        \"max_regparam\": 0.8,\n",
    "        \"min_elasticnet\": 0.5,\n",
    "        \"max_elasticnet\": 0.8\n",
    "    },\n",
    "    \"RandomForestClassifier\": {\n",
    "        \"model_name\": \"RandomForestClassifier\",\n",
    "        \"is_selected\": False,\n",
    "        \"num_estimators\": 100,\n",
    "        \"max_depth\": 10\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to create the selected model(s)\n",
    "def create_model(config, X_train, y_train):\n",
    "    # Parse prediction type\n",
    "    prediction_type = config[\"prediction_type\"]\n",
    "    models = []\n",
    "    \n",
    "    # If classification is required, select classification models\n",
    "    if prediction_type == \"classification\":\n",
    "        # Logistic Regression\n",
    "        if config[\"LogisticRegression\"][\"is_selected\"]:\n",
    "            log_reg_params = {\n",
    "                \"max_iter\": config[\"LogisticRegression\"][\"max_iter\"],\n",
    "                \"solver\": \"saga\",  # \"saga\" solver supports ElasticNet\n",
    "                \"penalty\": \"elasticnet\",\n",
    "                \"l1_ratio\": config[\"LogisticRegression\"][\"max_elasticnet\"],  # Use max elasticnet ratio\n",
    "                \"C\": 1 / config[\"LogisticRegression\"][\"min_regparam\"]  # Regularization parameter\n",
    "            }\n",
    "            models.append(LogisticRegression(**log_reg_params))\n",
    "\n",
    "        # Random Forest (only if selected)\n",
    "        if config[\"RandomForestClassifier\"][\"is_selected\"]:\n",
    "            rf_params = {\n",
    "                \"n_estimators\": config[\"RandomForestClassifier\"][\"num_estimators\"],\n",
    "                \"max_depth\": config[\"RandomForestClassifier\"][\"max_depth\"]\n",
    "            }\n",
    "            models.append(RandomForestClassifier(**rf_params))\n",
    "\n",
    "    return models\n",
    "\n",
    "# Example usage with valid dataset (ensure y has at least two classes)\n",
    "X = [[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]]  # Features\n",
    "y = [0, 1, 0, 1, 0]  # Labels (binary classification with two classes)\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Check if the dataset contains at least two classes\n",
    "if len(set(y_train)) < 2:\n",
    "    raise ValueError(\"The training data contains less than two classes. Logistic Regression requires at least two classes.\")\n",
    "\n",
    "# Create model(s) based on config\n",
    "models = create_model(config, X_train, y_train)\n",
    "\n",
    "# Example: Training the Logistic Regression model (if it is selected)\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"Model {model.__class__.__name__} trained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "461f5486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pavan Anbhule\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\pavan Anbhule\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\pavan Anbhule\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "18 fits failed out of a total of 36.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "18 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pavan Anbhule\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\pavan Anbhule\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"C:\\Users\\pavan Anbhule\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"C:\\Users\\pavan Anbhule\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1048, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\pavan Anbhule\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 864, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\pavan Anbhule\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 782, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\pavan Anbhule\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\pavan Anbhule\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\pavan Anbhule\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\pavan Anbhule\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\pavan Anbhule\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\pavan Anbhule\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\pavan Anbhule\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.36666667        nan 0.36666667        nan 0.38333333        nan\n",
      " 0.38333333        nan 0.43333333        nan 0.45              nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression\n",
      "Best Params: {'C': 10, 'max_iter': 100, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Validation Score: 0.4500\n",
      "Test Accuracy: 0.5000\n",
      "--------------------------------------------------\n",
      "Model: RandomForestClassifier\n",
      "Best Params: {'max_depth': 5, 'n_estimators': 200}\n",
      "Validation Score: 0.6000\n",
      "Test Accuracy: 0.5500\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 5) Run the fit and predict on each model – keep in mind that you need to do hyper parameter tuning\n",
    "#    i.e., use GridSearchCV\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Sample JSON Configuration\n",
    "config = {\n",
    "    \"hyperparameters\": {\n",
    "        \"search_method\": \"Grid Search\",\n",
    "        \"Grid Search\": {\n",
    "            \"is_selected\": True,\n",
    "            \"shuffle_grid\": True,\n",
    "            \"random_state\": 0,\n",
    "            \"cross_validation_strategy\": \"Time-based K-fold(with overlap)\",\n",
    "            \"Time-based K-fold(with overlap)\": {\n",
    "                \"is_selected\": True,\n",
    "                \"num_of_folds\": 3,\n",
    "                \"split_ratio\": 0.8,\n",
    "                \"stratified\": False\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"LogisticRegression\": {\n",
    "        \"model_name\": \"LogisticRegression\",\n",
    "        \"is_selected\": True,\n",
    "        \"max_iter\": [50, 100],\n",
    "        \"penalty\": [\"l2\", \"elasticnet\"],\n",
    "        \"C\": [0.1, 1.0, 10]\n",
    "    },\n",
    "    \"RandomForestClassifier\": {\n",
    "        \"model_name\": \"RandomForestClassifier\",\n",
    "        \"is_selected\": True,\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [5, 10, 20]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Sample Dataset\n",
    "X = np.random.rand(100, 5)  # Random features\n",
    "y = np.random.choice([0, 1], size=100)  # Binary target\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to perform Grid Search with hyperparameter tuning\n",
    "def hyperparameter_tuning(config, X_train, y_train, X_test, y_test):\n",
    "    models = []\n",
    "    results = []\n",
    "\n",
    "    # Time-based K-Fold Cross Validation\n",
    "    cv_strategy = None\n",
    "    if config[\"hyperparameters\"][\"Grid Search\"][\"Time-based K-fold(with overlap)\"][\"is_selected\"]:\n",
    "        num_of_folds = config[\"hyperparameters\"][\"Grid Search\"][\"Time-based K-fold(with overlap)\"][\"num_of_folds\"]\n",
    "        cv_strategy = TimeSeriesSplit(n_splits=num_of_folds)\n",
    "    \n",
    "    # Logistic Regression\n",
    "    if config[\"LogisticRegression\"][\"is_selected\"]:\n",
    "        param_grid = {\n",
    "            \"max_iter\": config[\"LogisticRegression\"][\"max_iter\"],\n",
    "            \"penalty\": config[\"LogisticRegression\"][\"penalty\"],\n",
    "            \"C\": config[\"LogisticRegression\"][\"C\"],\n",
    "            \"solver\": [\"saga\"]  # saga supports elasticnet\n",
    "        }\n",
    "        model = LogisticRegression()\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=cv_strategy, scoring=\"accuracy\", verbose=1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        models.append(grid_search.best_estimator_)\n",
    "        results.append((grid_search.best_estimator_, grid_search.best_params_, grid_search.best_score_))\n",
    "\n",
    "    # Random Forest\n",
    "    if config[\"RandomForestClassifier\"][\"is_selected\"]:\n",
    "        param_grid = {\n",
    "            \"n_estimators\": config[\"RandomForestClassifier\"][\"n_estimators\"],\n",
    "            \"max_depth\": config[\"RandomForestClassifier\"][\"max_depth\"]\n",
    "        }\n",
    "        model = RandomForestClassifier()\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=cv_strategy, scoring=\"accuracy\", verbose=1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        models.append(grid_search.best_estimator_)\n",
    "        results.append((grid_search.best_estimator_, grid_search.best_params_, grid_search.best_score_))\n",
    "\n",
    "    # Evaluate each model on test data\n",
    "    for model, best_params, best_score in results:\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Model: {model.__class__.__name__}\")\n",
    "        print(f\"Best Params: {best_params}\")\n",
    "        print(f\"Validation Score: {best_score:.4f}\")\n",
    "        print(f\"Test Accuracy: {acc:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run Hyperparameter Tuning\n",
    "hyperparameter_tuning(config, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b2d74cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Variable: petal_width\n",
      "Type of Model: regression\n",
      "Model Metrics (Regression):\n",
      "Mean Absolute Error (MAE): 0.2882\n",
      "Mean Squared Error (MSE): 0.1065\n",
      "R² Score: -0.0492\n"
     ]
    }
   ],
   "source": [
    "# 6) Log to the console the standard model metrics that apply\n",
    "\n",
    "from docx import Document\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "# Load and Parse JSON from Word Document\n",
    "doc_path = r\"C:\\Users\\pavan Anbhule\\Downloads\\DA_Assessment\\algoparams_from_ui1.docx\"  # Replace with the correct file path\n",
    "doc = Document(doc_path)\n",
    "json_text = \"\".join([p.text for p in doc.paragraphs])\n",
    "\n",
    "try:\n",
    "    data = json.loads(json_text)\n",
    "    target = data.get(\"design_state_data\", {}).get(\"target\", {}).get(\"target\", \"Not found\")\n",
    "    regression_type = data.get(\"design_state_data\", {}).get(\"target\", {}).get(\"type\", \"Not found\").lower()  # Lowercase for consistency\n",
    "    print(f\"Target Variable: {target}\")\n",
    "    print(f\"Type of Model: {regression_type}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(\"The content in the Word file is not valid JSON.\")\n",
    "    print(f\"Error Details: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Generate Sample Data\n",
    "X = np.random.rand(100, 5)  # Random features\n",
    "y = np.random.rand(100) if \"regression\" in regression_type else np.random.choice([0, 1], size=100)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and Evaluate Model\n",
    "if \"regression\" in regression_type:\n",
    "    # Linear Regression\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Compute Metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Log Metrics\n",
    "    print(\"Model Metrics (Regression):\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "elif \"classification\" in regression_type:\n",
    "    # Logistic Regression\n",
    "    model = LogisticRegression(max_iter=100)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Compute Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Log Metrics\n",
    "    print(\"Model Metrics (Classification):\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Unknown Model Type: {regression_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "453c6613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Variable: petal_width\n",
      "Task Type: regression\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No regression model selected in the JSON configuration.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 128\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown task type. Please update JSON configuration.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 128\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 96\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask Type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Validate JSON Configuration\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m \u001b[43mvalidate_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Generate Sample Data\u001b[39;00m\n\u001b[0;32m     99\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 26\u001b[0m, in \u001b[0;36mvalidate_json\u001b[1;34m(config, task_type)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m\"\"\"Validate JSON configuration to ensure at least one valid model is selected.\"\"\"\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinearRegression\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_selected\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo regression model selected in the JSON configuration.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogisticRegression\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_selected\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo classification model selected in the JSON configuration.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: No regression model selected in the JSON configuration."
     ]
    }
   ],
   "source": [
    "# 7) Please write generic code that can parse any JSON that follow this JSON format. So goal is you are using generic function in python.\n",
    "#  It will be most efficient if you use sklean pipelines for each stage namely\n",
    "#  a) feature handling part\n",
    "#  b) feature reduction part and\n",
    "#  c) model fit with grid search cv so that you can execute the pipeline object. \n",
    "#     For your testing try and change the fields in the JSON like say enable some algos setting ‘is_selected’ to true and now that algo \n",
    "#     should get executed when you run your script again. \n",
    "\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, mean_absolute_error, mean_squared_error, r2_score\n",
    "from docx import Document\n",
    "\n",
    "\n",
    "# Step 1: Load JSON from Word Document\n",
    "def load_json_from_docx(doc_path):\n",
    "    \"\"\"Load JSON from a Word document.\"\"\"\n",
    "    doc = Document(doc_path)\n",
    "    json_text = \"\".join([p.text for p in doc.paragraphs])\n",
    "    return json.loads(json_text)\n",
    "\n",
    "\n",
    "# Step 2: Validate JSON Configuration\n",
    "def validate_json(config, task_type):\n",
    "    \"\"\"Validate JSON configuration to ensure at least one valid model is selected.\"\"\"\n",
    "    if task_type == \"regression\" and not config.get(\"LinearRegression\", {}).get(\"is_selected\", False):\n",
    "        raise ValueError(\"No regression model selected in the JSON configuration.\")\n",
    "    if task_type == \"classification\" and not config.get(\"LogisticRegression\", {}).get(\"is_selected\", False):\n",
    "        raise ValueError(\"No classification model selected in the JSON configuration.\")\n",
    "\n",
    "\n",
    "# Step 3: Create Pipeline\n",
    "def create_pipeline(config, task_type):\n",
    "    \"\"\"Create a pipeline based on the JSON configuration.\"\"\"\n",
    "    steps = []\n",
    "\n",
    "    # Feature Handling\n",
    "    steps.append(('scaler', StandardScaler()))\n",
    "\n",
    "    # Feature Reduction\n",
    "    feature_reduction_config = config.get(\"feature_reduction\", {})\n",
    "    if feature_reduction_config.get(\"No Reduction\", {}).get(\"is_selected\", False):\n",
    "        print(\"No feature reduction applied.\")\n",
    "    elif feature_reduction_config.get(\"Correlation with target\", {}).get(\"is_selected\", False):\n",
    "        steps.append(('feature_selection', SelectKBest(score_func=f_classif,\n",
    "                                                       k=feature_reduction_config.get(\"Correlation with target\", {}).get(\"num_of_features_to_keep\", 5))))\n",
    "    elif feature_reduction_config.get(\"Principal Component Analysis\", {}).get(\"is_selected\", False):\n",
    "        steps.append(('pca', PCA(n_components=feature_reduction_config.get(\"Principal Component Analysis\", {}).get(\"num_of_features_to_keep\", 2))))\n",
    "\n",
    "    # Model Selection\n",
    "    model = None\n",
    "    grid_params = {}\n",
    "    if config.get(\"LogisticRegression\", {}).get(\"is_selected\", False) and task_type == \"classification\":\n",
    "        model = LogisticRegression()\n",
    "        grid_params = {\n",
    "            'model__C': np.linspace(\n",
    "                config[\"LogisticRegression\"][\"min_regparam\"],\n",
    "                config[\"LogisticRegression\"][\"max_regparam\"],\n",
    "                5\n",
    "            ),\n",
    "            'model__max_iter': [config[\"LogisticRegression\"][\"max_iter\"]]\n",
    "        }\n",
    "    elif config.get(\"LinearRegression\", {}).get(\"is_selected\", False) and task_type == \"regression\":\n",
    "        model = LinearRegression()\n",
    "        grid_params = {}  # No hyperparameter tuning needed for LinearRegression\n",
    "    elif config.get(\"RandomForestClassifier\", {}).get(\"is_selected\", False) and task_type == \"classification\":\n",
    "        model = RandomForestClassifier()\n",
    "        grid_params = {\n",
    "            'model__n_estimators': [50, 100, 150],\n",
    "            'model__max_depth': [5, 10, 20]\n",
    "        }\n",
    "\n",
    "    if not model:\n",
    "        raise ValueError(\"No valid model selected in the JSON configuration.\")\n",
    "\n",
    "    steps.append(('model', model))\n",
    "    pipeline = Pipeline(steps)\n",
    "\n",
    "    # Return pipeline and GridSearchCV\n",
    "    return GridSearchCV(pipeline, param_grid=grid_params, cv=5, scoring='accuracy' if task_type == 'classification' else 'neg_mean_squared_error', verbose=1)\n",
    "\n",
    "\n",
    "# Step 4: Main Function\n",
    "def main():\n",
    "    # Load JSON from Word document\n",
    "    doc_path = r\"C:\\Users\\pavan Anbhule\\Downloads\\DA_Assessment\\algoparams_from_ui1.docx\"  # Replace with your path\n",
    "    config = load_json_from_docx(doc_path)\n",
    "\n",
    "    # Extract Target and Task Type\n",
    "    target = config.get(\"design_state_data\", {}).get(\"target\", {}).get(\"target\", \"Not found\")\n",
    "    task_type = config.get(\"design_state_data\", {}).get(\"target\", {}).get(\"type\", \"\").lower()\n",
    "\n",
    "    print(f\"Target Variable: {target}\")\n",
    "    print(f\"Task Type: {task_type}\")\n",
    "\n",
    "    # Validate JSON Configuration\n",
    "    validate_json(config, task_type)\n",
    "\n",
    "    # Generate Sample Data\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(100, 5)  # Random features\n",
    "    y = np.random.choice([0, 1], size=100) if \"classification\" in task_type else np.random.rand(100)\n",
    "\n",
    "    # Split Data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create and Train Pipeline\n",
    "    grid_search = create_pipeline(config, task_type)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the Best Model\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    if \"classification\" in task_type:\n",
    "        print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    elif \"regression\" in task_type:\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        print(\"Regression Metrics:\")\n",
    "        print(f\"MAE: {mae:.4f}, MSE: {mse:.4f}, R²: {r2:.4f}\")\n",
    "    else:\n",
    "        print(\"Unknown task type. Please update JSON configuration.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15493816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
